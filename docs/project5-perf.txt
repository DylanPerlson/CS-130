Performance Log
================


Tarjan redundancy
---------------

**Theory**
We expect that there was an uneccesarrily high amount of Tarjan algorithm calls to find circular references. Because the tarjan algorithm iterates through every single cell in the workbook, it is a very time intensive algorithm. Before implementing any changes, we would call the tarjan algorithm every single time we called set_cell_contents(). So if we wanted to get the value of a single cell after moving 1000 cells, we would have to call tarjan 1000 times. We suspect if we move the tarjan algorithm to get_cell_value() instead, we would see massive speed increases because it would be called significantly less when setting large amounts of cells.

**Rationale**
As we can see in the below cProfiler data, the helper function for our tarjan algorithm is taking up a vast majority of the time for our performance test. This supports our hypothesis that the tarjan function is taking up a significant amount of time due to redundancies.

17738176 function calls (17700214 primitive calls) in 14.297 seconds

   Ordered by: internal time
   List reduced from 113 to 5 due to restriction <5>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
   499500    6.112    0.000    8.486    0.000 c:\Users\Dylan\Documents\GitHub\CS-130\sheets\workbook.py:1092(_tarjan_helper)
   498501    1.997    0.000    1.997    0.000 {method 'index' of 'list' objects}
     7884    1.286    0.000    3.831    0.000 C:\Users\Dylan\anaconda3\lib\site-packages\lark\parsers\earley.py:65(predict_and_complete)
      999    0.902    0.001    9.389    0.009 c:\Users\Dylan\Documents\GitHub\CS-130\sheets\workbook.py:1153(_tarjan) 
   938061    0.768    0.000    1.161    0.000 

**Outcome**
After we made changes to the tarjan function, we can see a massive increase in our performance test speed. Our previous test took 14.3 seconds to complete, and now it takes 5.2 seconds to compelte. This is an almost 3x speed improvement. Additionally, we can see from the cProfiler data below, that the Tarjan algorithm functions are no longer taking the majority of the time because they are being called significantly less.

13507542 function calls (13453596 primitive calls) in 5.187 seconds

   Ordered by: cumulative time
   List reduced from 135 to 10 due to restriction <10>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
     1000    0.009    0.000    5.160    0.005 c:\Users\Dylan\Documents\GitHub\CS-130\sheets\workbook.py:549(set_cell_contents)
4998/3000    0.008    0.000    5.093    0.002 c:\Users\Dylan\Documents\GitHub\CS-130\sheets\sheet.py:99(get_cell_value)
4997/3000    0.021    0.000    5.080    0.002 


Getting Cell References
----------------------

**Theory**
Previously When trying to improve our performance tests, we had noticed that using lark was consistantly one of the slowest functions to run. As such we thought that if we could remove unneccesary lark parsing, we might be able to get a large performance increase on large cell operations. As such, we noticed that when trying to get the cells that are called in a formula, we are parsing the cell contents to find the cell references. We suspect that the parsing of the cells could be done only once, and we can then store the cell references inside of the cell to be accessed at later times when needed.

**Rationale**
As we can see in the below cProfiler data, when doing our move_cells operation, the lark parser function is taking up the most of the time. Because we did not write the lark parsing funciton, we are not able to speed up the function call itself, so the only way that we would be able to increase lark parsing performance would be too call it less. Thus, the best thing that we can do to reduce our move_cell operation, is to remove any unnecesary lark calls.

 13791744 function calls (13749954 primitive calls) in 5.248 seconds

   Ordered by: cumulative time
   List reduced from 119 to 5 due to restriction <5>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.015    0.015    5.248    5.248 c:\Users\Dylan\Documents\GitHub\CS-130\sheets\workbook.py:73(move_cells)     2498    0.007    0.000    5.186    0.002 c:\Users\Dylan\Documents\GitHub\CS-130\sheets\sheet.py:113(_retrieve_cell_references)
     1996    0.002    0.000    5.155    0.003 C:\Users\Dylan\anaconda3\lib\site-packages\lark\lark.py:599(parse)      
     1996    0.010    0.000    5.152    0.003 C:\Users\Dylan\anaconda3\lib\site-packages\lark\parser_frontends.py:95(parse)
     1996    0.017    0.000    5.141    0.003 C:\Users\Dylan\anaconda3\lib\site-packages\lark\parsers\earley.py:249(parse)


**Outcome**
After we made changes to the retrieve_cell_reference() function, we were able to get a speed increase from 5.2 seconds to 2.6 seconds, a roughly 2x speed increase. This was because we were able to dramatically decrease our calls to the lark parser as can be seen in the cProfiler data below. The lark processer is no longer in the top 5 cummulative time cases.

7007102 function calls (6981237 primitive calls) in 2.652 seconds

   Ordered by: cumulative time
   List reduced from 135 to 5 due to restriction <5>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.010    0.010    2.652    2.652 c:\Users\Dylan\Documents\GitHub\CS-130\sheets\workbook.py:79(move_cells)     20oooo00    0.009    0.000    2.624    0.001 c:\Users\Dylan\Documents\GitHub\CS-130\sheets\sheet.py:43(set_cell_contents)
     2498    0.002    0.000    2.605    0.001 c:\Users\Dylan\Documents\GitHub\CS-130\sheets\sheet.py:111(_retrieve_cell_references)
2995/2001    0.005    0.000    2.603    0.001 c:\Users\Dylan\Documents\GitHub\CS-130\sheets\sheet.py:99(get_cell_value)
2995/2001    0.010    0.000    2.595    0.001 c:\Users\Dylan\Documents\GitHub\CS-130\sheets\cell.py:27(get_cell_value)

Updating Cell Dependencies
----------------------

**Theory**
Another issue that we were having was reaching the maximum python recursion depth very quickly when doing large cell movements. We believed that this was due to how we were notifying cells that they needed to be updated because a dependency of theirs was changed. Instead of updating the cells iteratively, we were updating the cells recursively.

**Rationale**
To test if this were the case, we stopped notifying cells that they needed to have their value updated. When we did this, we were able to run our program successfully without any maximum recursion depth issues. Thus, we knew that this was definitely the issue causing a maximum recursion depth failure.

**Outcome**
When we changed to cell update dependencies function to be iteratively, we were able to run our program normally for large cell movements without reaching any recursion depth limits. This did not speed up our code at all, so this was not a speed performance increase, but it did increase our ability to handle large areas of cells without failing. So it was instead a large usability performance increase because it increased the amount of cells that our program was able to handle without breaking.